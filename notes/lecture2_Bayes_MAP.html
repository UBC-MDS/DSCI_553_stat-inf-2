

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Lecture 2 - Conditional Probabilities, Bayes’ Rule, and Maximum a Posteriori Estimation &#8212; DSCI 553 - Statistical Inference and Computation II</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notes/lecture2_Bayes_MAP';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Lecture 3 - Bayesian Statistics in Action: The Beta-Binomial Model" href="lecture3_beta_binomial_Bayesian_modelling.html" />
    <link rel="prev" title="Lecture 1 - Frequentist and Bayesian Overview, Probabilistic Generative Models, and Stan" href="lecture1_intro_and_stan.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/UBC_MDS_logo.png" class="logo__image only-light" alt="DSCI 553 - Statistical Inference and Computation II - Home"/>
    <script>document.write(`<img src="../_static/UBC_MDS_logo.png" class="logo__image only-dark" alt="DSCI 553 - Statistical Inference and Computation II - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Welcome to DSCI 553: Statistical Inference and Computation II
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../lecture-learning-objectives.html">Lecture Learning Objectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture1_intro_and_stan.html">Lecture 1 - Frequentist and Bayesian Overview, Probabilistic Generative Models, and <code class="docutils literal notranslate"><span class="pre">Stan</span></code></a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Lecture 2 - Conditional Probabilities, Bayes’ Rule, and Maximum a Posteriori Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture3_beta_binomial_Bayesian_modelling.html">Lecture 3 - Bayesian Statistics in Action: The Beta-Binomial Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture4_MCMC_Poisson_Gamma_Normal.html">Lecture 4 - Markov Chain Monte Carlo, <code class="docutils literal notranslate"><span class="pre">Stan</span></code>, and Complex Bayesian Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture5_hypothesis_testing_intro_regression.html">Lecture 5 - Bayesian Normal Linear Regression and Hypothesis Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture6_binary_logistic_regression.html">Lecture 6 - Bayesian Binary Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture7_hierarchical_models.html">Lecture 7 - Bayesian Hierarchical Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture8_model_diagnostics.html">Lecture 8 - More Hierarchical Modelling and MCMC Diagnostics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorial</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="MCMC_tutorial.html">Insights of Markov Chain Monte Carlo via the Gamma-Poisson Model</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="appendix-dist-cheatsheet.html">Distribution Cheatsheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-greek-alphabet.html">Greek Alphabet</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix-bayesian-workflow.html">The Bayesian Workflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/UBC-MDS/DSCI_553_stat-inf-2" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/UBC-MDS/DSCI_553_stat-inf-2/issues/new?title=Issue%20on%20page%20%2Fnotes/lecture2_Bayes_MAP.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notes/lecture2_Bayes_MAP.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 2 - Conditional Probabilities, Bayes’ Rule, and Maximum a Posteriori Estimation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-learning-objectives">Today’s Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-next">0. What is next?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">1. Conditional Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule-fundamentals">2. Bayes’ Rule Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-statistical-inquiry">2.1. Main Statistical Inquiry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-data">2.2. The Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior">2.3. The Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability-and-likelihood">2.4. Conditional Probability and Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-constant">2.5. Normalizing Constant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule-and-posterior-probability">2.6. Bayes’ Rule and Posterior Probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-of-independent-events">2.7. Rule of Independent Events</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimation-map-and-maximum-likelihood-estimation-mle">3. Maximum a Posteriori Estimation (MAP) and Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up">4. Wrapping Up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-puzzle-if-time-permits">5. Probability Puzzle (if time permits)</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-2-conditional-probabilities-bayes-rule-and-maximum-a-posteriori-estimation">
<h1>Lecture 2 - Conditional Probabilities, Bayes’ Rule, and Maximum a Posteriori Estimation<a class="headerlink" href="#lecture-2-conditional-probabilities-bayes-rule-and-maximum-a-posteriori-estimation" title="Permalink to this heading">#</a></h1>
<section id="today-s-learning-objectives">
<h2>Today’s Learning Objectives<a class="headerlink" href="#today-s-learning-objectives" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Illustrate the concept of conditional probability.</p></li>
<li><p>Apply the Bayes rule to probabilistic inquiries.</p></li>
<li><p>Introduce the concept of maximum a posteriori (MAP) estimation.</p></li>
<li><p>Relate MAP with maximum likelihood (MLE) estimation.</p></li>
</ol>
</section>
<section id="what-is-next">
<h2>0. What is next?<a class="headerlink" href="#what-is-next" title="Permalink to this heading">#</a></h2>
<p>Previously, we discussed <strong>the drawbacks frequentist inference could have</strong>. Hence, we concluded we might need a better approach for complex systems while leaving some wiggle room to randomness (i.e., a <strong>probabilistic generative model</strong>).</p>
<p>Furthermore, our modelling might need to incorporate <strong>latent variables</strong> if we cannot directly observe our variable of interest.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Both types of inference, frequentist and Bayesian, could use <strong>probabilistic generative models</strong> but the difference lies in their <strong>statistical reasoning</strong>.</p>
</div>
<div class="exercise admonition" id="lecture2-q1">

<p class="admonition-title"><span class="caption-number">Exercise 4 </span></p>
<section id="exercise-content">
<p><strong>Answer TRUE or FALSE:</strong></p>
<p><strong>Bayesians assume that population parameters are random</strong>, unlike frequentists who assume these parameters are fixed.</p>
<p><strong>A.</strong> TRUE</p>
<p><strong>B.</strong> FALSE</p>
</section>
</div>
<p>Let us recall the concept of <strong>recursive updating</strong>.</p>
<br>
<center><img width="700" src="https://www.bayesrulesbook.com/chapters/figs/chapter_1/bayes_diagram.png"/></center>
<p><em>Source: <a class="reference external" href="https://www.bayesrulesbook.com/chapter-1.html">Johnson et al. (2021)</a></em></p>
</section>
<section id="conditional-probability">
<h2>1. Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this heading">#</a></h2>
<p>This lecture will retake different probability concepts you might already be familiar with (<strong>from DSCI 551</strong>). Nonetheless, these concepts will pave the way to a key concept in this inferential course: <strong>the Bayes’ rule</strong>.</p>
<center>        
<img src="https://logos-world.net/wp-content/uploads/2020/09/Tinder-Logo.png" width="300" height="170"/>
</center>
<p>To illustrate these concepts, <strong>for the single population living in Vancouver looking for a partner</strong>, let us suppose we want to estimate the probability of finding a partner <strong>given</strong> they have used the Tinder app.</p>
<p>We collect a random sample from this population of interest, follow them up for two months, and get our study results involving whether they found a partner or not, along with whether they used Tinder (<strong>since we started monitoring them</strong>).</p>
<p>Note the following:</p>
<ul class="simple">
<li><p>We have a sample <span class="math notranslate nohighlight">\(S\)</span>, whose <strong>cardinality</strong> (<em>i.e., number of subjects within the dataset</em>) is <span class="math notranslate nohighlight">\(\mid S \mid\)</span>. <strong>This cardinality is basically our sample size.</strong></p></li>
<li><p>Let <span class="math notranslate nohighlight">\(L\)</span> be the following <strong>primary</strong> event of interest: <strong>they found a partner</strong> (<font color='red'><span class="math notranslate nohighlight">\(L\)</span> for love </font>). Then, with <span class="math notranslate nohighlight">\(\mid L \mid\)</span> being the cardinality of this subset of sampled people, its probability will be defined by</p></li>
</ul>
<div class="math notranslate nohighlight">
\[P(L) = \frac{\mid L \mid}{\mid S \mid}.\]</div>
<p>Graphically, the event <span class="math notranslate nohighlight">\(L\)</span> can be represented as:</p>
<figure class="align-default" id="venn-a">
<a class="reference internal image-reference" href="../_images/venn_A.jpg"><img alt="../_images/venn_A.jpg" src="../_images/venn_A.jpg" style="height: 350px;" /></a>
</figure>
<p>Calculating <span class="math notranslate nohighlight">\(P(L)\)</span> is really straightforward using our sampled data. However, in many instances, we would like to update our sampling space based on a <strong>secondary</strong> event of interest.</p>
<p>That is right! This other event of interest is <strong>using Tinder to find a partner</strong>.</p>
<p>Thus, let us define <span class="math notranslate nohighlight">\(T\)</span> as: <strong>using Tinder to find a partner</strong>. Then, with <span class="math notranslate nohighlight">\(\mid T \mid\)</span> being the cardinality of this subset of sampled people, its probability will be defined by</p>
<div class="math notranslate nohighlight">
\[P(T) = \frac{\mid T \mid}{\mid S \mid}.\]</div>
<p>Graphically, the event <span class="math notranslate nohighlight">\(T\)</span> can be represented as:</p>
<figure class="align-default" id="venn-b">
<a class="reference internal image-reference" href="../_images/venn_B.jpg"><img alt="../_images/venn_B.jpg" src="../_images/venn_B.jpg" style="height: 350px;" /></a>
</figure>
<p>Moreover, assume we have subjects in our sample who found partner <strong>and</strong> used Tinder; i.e., there is an intersection of events: <span class="math notranslate nohighlight">\(L \cap T\)</span>. Then, with <span class="math notranslate nohighlight">\(\mid L \cap T \mid\)</span> being the cardinality of this subset of sampled people, its probability will be defined by</p>
<div class="math notranslate nohighlight">
\[P(L \cap T) = \frac{\mid L \cap T \mid}{\mid S \mid}\]</div>
<p>Graphically, the event <span class="math notranslate nohighlight">\(L \cap T\)</span> (i.e., <span class="math notranslate nohighlight">\(L\)</span> <strong>and</strong> <span class="math notranslate nohighlight">\(T\)</span>) can be represented as:</p>
<figure class="align-default" id="venn-c">
<a class="reference internal image-reference" href="../_images/venn_C.jpg"><img alt="../_images/venn_C.jpg" src="../_images/venn_C.jpg" style="height: 350px;" /></a>
</figure>
<p><strong>From the perspective of a Tinder user who is using the app the find a partner</strong>, recall our initial objective: <em>we want to infer the probability of finding a partner <strong>given</strong> we use the Tinder app in Vancouver</em>. In probability notation, this is coded as a conditional probability:</p>
<div class="math notranslate nohighlight">
\[P(L \mid T) = \text{Probability of finding a partner GIVEN we use Tinder}\]</div>
<p>We zoom in the previous figure:</p>
<figure class="align-default" id="venn-d">
<a class="reference internal image-reference" href="../_images/venn_D.jpg"><img alt="../_images/venn_D.jpg" src="../_images/venn_D.jpg" style="height: 350px;" /></a>
</figure>
<p><strong>Let us focus on the areas indicating the event <span class="math notranslate nohighlight">\(T\)</span> and intersection <span class="math notranslate nohighlight">\(L \cap T\)</span> (i.e., <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(T\)</span>).</strong> Thus, how can we compute <span class="math notranslate nohighlight">\(P(L \mid T)\)</span>?</p>
<p>Our previous probability calculations will help us as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(L \mid T) &amp;= \frac{\mid L \cap T \mid}{\mid T \mid} \\
&amp;= \frac{\frac{\mid L \cap T \mid}{\mid S \mid}}{\frac{\mid T \mid}{\mid S \mid}} &amp;&amp; \text{diving numerator and denominator over } \mid S \mid \\
&amp;= \frac{P(L \cap T)}{P(T)}
\end{align*}\end{split}\]</div>
<p>We just derived the conditional probability formula for <span class="math notranslate nohighlight">\(P(L \mid T)\)</span>.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p><span class="math notranslate nohighlight">\(P(L \mid T)\)</span> will be the <strong>ultimate finding</strong> in this lecture.</p>
</div>
<div class="important admonition">
<p class="admonition-title">Formal Definition of Conditional Probability</p>
<p>Let <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(T\)</span> be two events of interest within the sample <span class="math notranslate nohighlight">\(S\)</span>, and <span class="math notranslate nohighlight">\(P(T) &gt; 0\)</span>, then the <strong>conditional probability</strong> of <span class="math notranslate nohighlight">\(L\)</span> given <span class="math notranslate nohighlight">\(T\)</span> is defined as:</p>
<div class="math notranslate nohighlight" id="equation-cond-prob">
<span class="eqno">(3)<a class="headerlink" href="#equation-cond-prob" title="Permalink to this equation">#</a></span>\[P(L \mid T) = \frac{P(L \cap T)}{P(T)}\]</div>
</div>
<div class="exercise admonition" id="lecture2-q2">

<p class="admonition-title"><span class="caption-number">Exercise 5 </span></p>
<section id="exercise-content">
<p>What is going on in the equation <a class="reference internal" href="#equation-cond-prob">(3)</a>? <strong>Select the correct answer.</strong></p>
<p><strong>A.</strong> Event <span class="math notranslate nohighlight">\(L\)</span> becomes the new sample space.</p>
<p><strong>B.</strong> Event <span class="math notranslate nohighlight">\(T\)</span> becomes the new sample space.</p>
<p><strong>C.</strong> The sample space <span class="math notranslate nohighlight">\(S\)</span> remains the same under condition <span class="math notranslate nohighlight">\(T\)</span>.</p>
</section>
</div>
</section>
<section id="bayes-rule-fundamentals">
<h2>2. Bayes’ Rule Fundamentals<a class="headerlink" href="#bayes-rule-fundamentals" title="Permalink to this heading">#</a></h2>
<p>Let us proceed with our Tinder study! We will put in practice the workflow of the <strong>recursive updating</strong> in a first iteration.</p>
<br>
<center><img width="700" src="https://www.bayesrulesbook.com/chapters/figs/chapter_1/bayes_diagram.png"/></center>
<p><em>Source: <a class="reference external" href="https://www.bayesrulesbook.com/chapter-1.html">Johnson et al. (2021)</a></em></p>
<section id="main-statistical-inquiry">
<h3>2.1. Main Statistical Inquiry<a class="headerlink" href="#main-statistical-inquiry" title="Permalink to this heading">#</a></h3>
<p>We must set up our main inquiry if we implement the Bayesian recursive updating. Suppose your are a social local researcher and Tinder enthusiast:</p>
<blockquote>
<div><p>Via our <strong>observed data</strong> (a.k.a, <strong>observed evidence</strong>) and <strong>prior social research</strong>, we want to make inference on the probability of finding a partner <strong>given</strong> we used the Tinder app in Vancouver, i.e., <span class="math notranslate nohighlight">\(P(L \mid T)\)</span>.</p>
</div></blockquote>
</section>
<section id="the-data">
<h3>2.2. The Data<a class="headerlink" href="#the-data" title="Permalink to this heading">#</a></h3>
<p>Let us start with the data. Suppose we collected the data from the population of interest (<strong>adult single people in Vancouver</strong>) via a sample of size <code class="docutils literal notranslate"><span class="pre">100</span></code>. The dataset <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code> has two variables of interest: <code class="docutils literal notranslate"><span class="pre">tinder</span></code> (<code class="docutils literal notranslate"><span class="pre">User</span></code> or <code class="docutils literal notranslate"><span class="pre">Non-User</span></code>) and <code class="docutils literal notranslate"><span class="pre">rel_status</span></code> (<code class="docutils literal notranslate"><span class="pre">Partnered</span></code> or <code class="docutils literal notranslate"><span class="pre">Non-Partnered</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.matrix.max.rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">janitor</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">tidyverse</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">scales</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attaching package: ‘janitor’
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The following objects are masked from ‘package:stats’:

    chisq.test, fisher.test
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>── <span class=" -Color -Color-Bold">Attaching core tidyverse packages</span> ──────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 2.0.0 ──
<span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">dplyr    </span> 1.1.4     <span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">readr    </span> 2.1.5
<span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">forcats  </span> 1.0.0     <span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">stringr  </span> 1.5.1
<span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">ggplot2  </span> 3.5.1     <span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">tibble   </span> 3.2.1
<span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">lubridate</span> 1.9.3     <span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">tidyr    </span> 1.3.1
<span class=" -Color -Color-Green">✔</span> <span class=" -Color -Color-Blue">purrr    </span> 1.0.2     
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>── <span class=" -Color -Color-Bold">Conflicts</span> ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
<span class=" -Color -Color-Red">✖</span> <span class=" -Color -Color-Blue">dplyr</span>::<span class=" -Color -Color-Green">filter()</span> masks <span class=" -Color -Color-Blue">stats</span>::filter()
<span class=" -Color -Color-Red">✖</span> <span class=" -Color -Color-Blue">dplyr</span>::<span class=" -Color -Color-Green">lag()</span>    masks <span class=" -Color -Color-Blue">stats</span>::lag()
<span class=" -Color -Color-Cyan">ℹ</span> Use the conflicted package (<span class=" -Color -Color-Blue">&lt;http://conflicted.r-lib.org/&gt;</span>) to force all conflicts to become errors
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Attaching package: ‘scales’
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The following object is masked from ‘package:purrr’:

    discard
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The following object is masked from ‘package:readr’:

    col_factor
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">tinder_obs_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read_csv</span><span class="p">(</span><span class="s">&quot;../data/tinder_obs_sample.csv&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">show_col_types</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>
<span class="n">tinder_obs_sample</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A spec_tbl_df: 100 × 2</caption>
<thead>
	<tr><th scope=col>tinder</th><th scope=col>rel_status</th></tr>
	<tr><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th></tr>
</thead>
<tbody>
	<tr><td>User    </td><td>Non-Partnered</td></tr>
	<tr><td>User    </td><td>Non-Partnered</td></tr>
	<tr><td>Non-User</td><td>Partnered    </td></tr>
	<tr><td>⋮</td><td>⋮</td></tr>
	<tr><td>User    </td><td>Non-Partnered</td></tr>
	<tr><td>Non-User</td><td>Partnered    </td></tr>
	<tr><td>User    </td><td>Partnered    </td></tr>
</tbody>
</table>
</div></div>
</div>
<p>We will follow the same flow you can find in <a class="reference external" href="https://www.bayesrulesbook.com/chapter-2#michelle-simple">Chapter 2 of Bayes Rules!</a> to explain the core Bayesian concepts (<strong>Example: Pop vs soda vs coke</strong>). But firstly, stacked bar charts by proportions can give us a useful graphical insight.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.height</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">12</span><span class="p">)</span>

<span class="c1"># Proportions summary</span>
<span class="n">tinder_sample.prop.summary</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">as.data.frame</span><span class="p">(</span><span class="nf">xtabs</span><span class="p">(</span>
<span class="w">  </span><span class="o">~</span><span class="w"> </span><span class="n">rel_status</span><span class="w"> </span><span class="o">+</span>
<span class="w">    </span><span class="n">tinder</span><span class="p">,</span>
<span class="w">  </span><span class="n">tinder_obs_sample</span>
<span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">rowSums</span><span class="p">(</span><span class="nf">xtabs</span><span class="p">(</span>
<span class="w">  </span><span class="o">~</span><span class="w"> </span><span class="n">rel_status</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">tinder</span><span class="p">,</span>
<span class="w">  </span><span class="n">tinder_obs_sample</span>
<span class="p">)),</span><span class="w"> </span><span class="n">responseName</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;prop&quot;</span><span class="p">)</span>

<span class="c1"># Stacked bar charts</span>
<span class="n">tinder_sample.stacked.bars</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">ggplot</span><span class="p">(</span><span class="n">tinder_sample.prop.summary</span><span class="p">,</span><span class="w"> </span><span class="nf">aes</span><span class="p">(</span>
<span class="w">  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rel_status</span><span class="p">,</span>
<span class="w">  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prop</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tinder</span>
<span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_bar</span><span class="p">(</span><span class="n">stat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;identity&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">linewidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.7</span><span class="p">,</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;black&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">geom_text</span><span class="p">(</span><span class="nf">aes</span><span class="p">(</span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">ifelse</span><span class="p">(</span><span class="n">prop</span><span class="w"> </span><span class="o">&gt;=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="nf">paste0</span><span class="p">(</span><span class="nf">sprintf</span><span class="p">(</span><span class="s">&quot;%.0f&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">prop</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="m">100</span><span class="p">),</span><span class="w"> </span><span class="s">&quot;%&quot;</span><span class="p">),</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">)),</span>
<span class="w">    </span><span class="n">position</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">position_stack</span><span class="p">(</span><span class="n">vjust</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">),</span><span class="w"> </span><span class="n">colour</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;firebrick3&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">fontface</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">9</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">percent_format</span><span class="p">())</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">labs</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Percentage&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Relationship Status&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">ggtitle</span><span class="p">(</span><span class="s">&quot;Stacked Bar Charts of Observed Tinder Sample&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">theme</span><span class="p">(</span>
<span class="w">    </span><span class="n">plot.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">24</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text.x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">17</span><span class="p">,</span><span class="w"> </span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.text.y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">17</span><span class="p">,</span><span class="w"> </span><span class="n">angle</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">),</span>
<span class="w">    </span><span class="n">axis.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">),</span>
<span class="w">    </span><span class="n">legend.text</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">,</span><span class="w"> </span><span class="n">margin</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">margin</span><span class="p">(</span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">unit</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;cm&quot;</span><span class="p">)),</span>
<span class="w">    </span><span class="n">legend.title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">element_text</span><span class="p">(</span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">21</span><span class="p">,</span><span class="w"> </span><span class="n">face</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;bold&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="p">)</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">guides</span><span class="p">(</span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">guide_legend</span><span class="p">(</span><span class="n">title</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Tinder Status&quot;</span><span class="p">))</span><span class="w"> </span><span class="o">+</span>
<span class="w">  </span><span class="nf">scale_fill_brewer</span><span class="p">(</span><span class="n">palette</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Blues&quot;</span><span class="p">)</span>

<span class="n">tinder_sample.stacked.bars</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<a class="reference internal image-reference" href="../_images/0d2a8a66c3c98d79c43137418e8a4e940fc4cab6f209f8c05ede186b3bde4e83.png"><img alt="../_images/0d2a8a66c3c98d79c43137418e8a4e940fc4cab6f209f8c05ede186b3bde4e83.png" src="../_images/0d2a8a66c3c98d79c43137418e8a4e940fc4cab6f209f8c05ede186b3bde4e83.png" style="width: 720px; height: 480px;" /></a>
</div>
</div>
<p><strong>From our observed evidence</strong>, note that the use of Tinder does not look too promising.</p>
</section>
<section id="the-prior">
<h3>2.3. The Prior<a class="headerlink" href="#the-prior" title="Permalink to this heading">#</a></h3>
<p>Our <strong>primary</strong> event of interest would be <span class="math notranslate nohighlight">\(L\)</span> (<strong>they found a partner</strong>). Suppose you found a <strong>prior Vancouver observational study</strong> of size <code class="docutils literal notranslate"><span class="pre">200</span></code> called <code class="docutils literal notranslate"><span class="pre">prior_sample</span></code>. Nevertheless, they only observed whether the adult subjects were partnered or not.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note this <code class="docutils literal notranslate"><span class="pre">prior_sample</span></code> in our Bayesian inference <strong>will give us valuable PRIOR insights into how Vancouver’s population behaves in terms of whether they are more prone to be in a relationship or not</strong>. This is quite important for us to know, besides using Tinder to be in a relationship!</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">prior_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read_csv</span><span class="p">(</span><span class="s">&quot;../data/prior_sample.csv&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">show_col_types</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>
<span class="n">prior_sample</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span>
<span class="w">  </span><span class="nf">tabyl</span><span class="p">(</span><span class="n">rel_status</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span>
<span class="w">  </span><span class="nf">adorn_totals</span><span class="p">(</span><span class="s">&quot;row&quot;</span><span class="p">)</span><span class="w"> </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tabyl: 3 × 3</caption>
<thead>
	<tr><th></th><th scope=col>rel_status</th><th scope=col>n</th><th scope=col>percent</th></tr>
	<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>Non-Partnered</td><td> 84</td><td>0.42</td></tr>
	<tr><th scope=row>2</th><td>Partnered    </td><td>116</td><td>0.58</td></tr>
	<tr><th scope=row>3</th><td>Total        </td><td>200</td><td>1.00</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Now, <strong>suppose we want to infer the relationship status of adults in Vancouver</strong>; our prior probability model (determined by the <code class="docutils literal notranslate"><span class="pre">prior_sample</span></code>) provides these probabilities:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
P_{prior}(L) = 0.58 \\
P_{prior}(L^c) = 1 - 0.58 = 0.42
\end{gather*}\end{split}\]</div>
<p>Note these prior probabilities add up to one, i.e., we are considering all possible outcomes in <span class="math notranslate nohighlight">\(L\)</span> (<strong>which is binary in this case!</strong>). It is more probable that an adult person is partnered in this city, as we can see from the <strong>sampled</strong> prior probabilities!</p>
</section>
<section id="conditional-probability-and-likelihood">
<h3>2.4. Conditional Probability and Likelihood<a class="headerlink" href="#conditional-probability-and-likelihood" title="Permalink to this heading">#</a></h3>
<p>We will get even more information from <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code> by taking into account the use of Tinder (i.e., event <span class="math notranslate nohighlight">\(T\)</span>). <strong>This data incorporation will determine whether the use of Tinder is more related to people that end up partnered compared to those who remain single (as shown in our previous stacked bar charts).</strong> We need to obtain these conditional probabilities:</p>
<div class="math notranslate nohighlight">
\[P(T \mid L) = \frac{P(T \cap L)}{P(L)} \quad \text{and} \quad P(T \mid L^c) = \frac{P(T \cap L^c)}{P(L^c)}\]</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We need to clarify that <span class="math notranslate nohighlight">\(P(T \mid L) \neq P(L \mid T)\)</span>. <strong><span class="math notranslate nohighlight">\(P(L \mid T)\)</span> is our ultimate inferential result according to the main statistical inquiry.</strong></p>
</div>
<p><strong>How can we compute <span class="math notranslate nohighlight">\(P(T \mid L)\)</span> and <span class="math notranslate nohighlight">\(P(T \mid L^c)\)</span>?</strong> We need the following absolute frequencies:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">tinder_obs_sample</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span>
<span class="w">  </span><span class="nf">tabyl</span><span class="p">(</span><span class="n">rel_status</span><span class="p">,</span><span class="w"> </span><span class="n">tinder</span><span class="p">)</span><span class="w"> </span><span class="o">|&gt;</span><span class="w"> </span>
<span class="w">  </span><span class="nf">adorn_totals</span><span class="p">(</span><span class="s">&quot;row&quot;</span><span class="p">)</span><span class="w"> </span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><table class="dataframe">
<caption>A tabyl: 3 × 3</caption>
<thead>
	<tr><th></th><th scope=col>rel_status</th><th scope=col>Non-User</th><th scope=col>User</th></tr>
	<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>
</thead>
<tbody>
	<tr><th scope=row>1</th><td>Non-Partnered</td><td> 4</td><td>58</td></tr>
	<tr><th scope=row>2</th><td>Partnered    </td><td>25</td><td>13</td></tr>
	<tr><th scope=row>3</th><td>Total        </td><td>29</td><td>71</td></tr>
</tbody>
</table>
</div></div>
</div>
<p>Hence:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
P(T \mid L) = \frac{P(T \cap L)}{P(L)} = \frac{\frac{13}{100}}{\frac{25 + 13}{100}} = 0.34 \\
P(T \mid L^c) = \frac{P(T \cap L^c)}{P(L^c)} = \frac{\frac{58}{100}}{\frac{4 + 58}{100}} = 0.94
\end{gather*}\end{split}\]</div>
<p>These two results can be found in our stacked bar charts. We can interpret them as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(T \mid L)\)</span>: <strong>IF</strong> a person ends up partnered, there is a 34% chance they used Tinder.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(T \mid L^c)\)</span>: <strong>IF</strong> a person remains single, there is a 94% chance they used Tinder.</p></li>
</ul>
<p>These two previous conditional probabilities are the evidence provided by our <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code>, which incorporates both events of interest: <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(T\)</span>. According to these results, a single person in Vancouver who uses Tinder is more <strong>LIKELY</strong> to remain single than end up partnered.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p><strong>Possibly</strong>, we might be confused about whether our study is actually giving us <span class="math notranslate nohighlight">\(P(L \mid T)\)</span> instead of <span class="math notranslate nohighlight">\(P(T \mid L)\)</span>. Therefore, we might be tempted to say:</p>
<blockquote>
<div><p><em>OK, we do not need to use any Bayesian reasoning with prior knowledge since our study suffices! Then, we would solve our main statistical right away.</em></p>
</div></blockquote>
<p>However, <strong>that is not the case in the Bayesian world</strong>.</p>
</div>
<p>Having said that, the <strong>“more likely”</strong> idea paves the way to another core concept in Bayesian inference: <strong>the likelihood function</strong>.</p>
<p>We use the following notation:</p>
<div class="math notranslate nohighlight">
\[\mathscr{l}(L \mid T) = P(T \mid L) \quad \text{and} \quad \mathscr{l}(L^c \mid T) = P(T \mid L^c)\]</div>
<p>How to interpret <span class="math notranslate nohighlight">\(\mathscr{l}(L \mid T)\)</span> and <span class="math notranslate nohighlight">\(\mathscr{l}(L^c \mid T)\)</span>?</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathscr{l}(L \mid T) = 0.34\)</span> indicates how likely a person will end up partenered if we primarily know they use Tinder. <strong>The use of Tinder is our primary observed evidence.</strong></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathscr{l}(L^c \mid T) = 0.94\)</span> indicates how likely a person will remain single if we primarily know they use Tinder. <strong>The use of Tinder is our primary observed evidence.</strong></p></li>
</ul>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>A likelihood function is <strong>NOT</strong> a probability function, thus it does <strong>NOT</strong> need to add up to one.</p>
</div>
<p>The following table summarizes our event of interest <span class="math notranslate nohighlight">\(L\)</span> (being partenered or not):</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Event</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L^c\)</span></p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Prior</p></td>
<td><p>0.58</p></td>
<td><p>0.42</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Likelihood</p></td>
<td><p>0.34</p></td>
<td><p>0.94</p></td>
<td><p>1.28</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Note the data (likelihood of 0.94) suggests that using Tinder to to find a partner in Vancouver is not the best idea! This is the evidence provided by our <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code>.</p>
</div>
</section>
<section id="normalizing-constant">
<h3>2.5. Normalizing Constant<a class="headerlink" href="#normalizing-constant" title="Permalink to this heading">#</a></h3>
<p>We are almost getting into the Bayes’ rule and posterior probabilities to solve our inquiry.</p>
<p>Now, we proceed to a concept called the <strong>normalizing constant</strong>. A normalizing constant refers to the marginal probability of using Tinder <strong>regardless</strong> we end up partnered or not. Let us find this normalizing constant via <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code> and <code class="docutils literal notranslate"><span class="pre">prior_sample</span></code>.</p>
<p>In this specific case, for our ultimate goal <span class="math notranslate nohighlight">\(P(L \mid T)\)</span>, check the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(L \mid T) &amp;= \frac{P(L \cap T)}{P(T)} \\
&amp;= \frac{P_{prior}(L) \times P(T \mid L)}{P(T)} \\
&amp;= \frac{P_{prior}(L) \times \mathscr{l}(L \mid T)}{P(T)} \\
&amp;= \frac{0.58 \times 0.34}{P(T)},
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(P_{prior}(L) = 0.58\)</span> is the estimated proportion of partnered adults in Vancouver coming from the <code class="docutils literal notranslate"><span class="pre">prior_sample</span></code>, and <span class="math notranslate nohighlight">\(\mathscr{l}(L \mid T) = 0.34\)</span> indicates how likely a person will end up partenered if we primarily know they use Tinder (coming from <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code>).</p>
<p>Nevertheless, <span class="math notranslate nohighlight">\(P(T)\)</span> is missing and we call it the normalizing constant. We can easily obtain it through the <strong>Law of Total Probability</strong> (note the tweak in the second line to <strong>observed likelihood values</strong>!):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(T) &amp;= [P(T \mid L) \times P_{prior}(L)] + [P(T \mid L^c) \times P_{prior}(L^c)]  \\
&amp;= [\mathscr{l}(L \mid T) \times P_{prior}(L)] + [\mathscr{l}(L^c \mid T) \times P_{prior}(L^c)] \\
&amp;= (0.34 \times 0.58) + (0.94 \times 0.42) \\
&amp;= 0.592.
\end{align*}\end{split}\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p><span class="math notranslate nohighlight">\(P_{prior}(L)\)</span> and <span class="math notranslate nohighlight">\(P_{prior}(L^c)\)</span> come from <code class="docutils literal notranslate"><span class="pre">prior_sample</span></code>, whereas <span class="math notranslate nohighlight">\(\mathscr{l}(L \mid T)\)</span> and <span class="math notranslate nohighlight">\(\mathscr{l}(L^c \mid T)\)</span> come <code class="docutils literal notranslate"><span class="pre">tinder_obs_sample</span></code>.</p>
</div>
</section>
<section id="bayes-rule-and-posterior-probability">
<h3>2.6. Bayes’ Rule and Posterior Probability<a class="headerlink" href="#bayes-rule-and-posterior-probability" title="Permalink to this heading">#</a></h3>
<p>Using our previous computation, we plug in <span class="math notranslate nohighlight">\(P(T)\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(L \mid T) = \frac{0.58 \times 0.34}{0.592} = 0.33.\]</div>
<p>We can do something analogous with <span class="math notranslate nohighlight">\(P(L^c \mid T)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(L^c \mid T) &amp;= \frac{P(L^c \cap T)}{P(T)} \\
&amp;= \frac{P_{prior}(L^c) \times P(T \mid L^c)}{P(T)} \\
&amp;= \frac{P_{prior}(L^c) \times \mathscr{l}(L^c \mid T)}{P(T)} \\
&amp;= \frac{0.42 \times 0.94}{0.592} = 0.67.
\end{align*}\end{split}\]</div>
<p><strong>Note this is the complement of <span class="math notranslate nohighlight">\(P(L \mid T)\)</span>.</strong></p>
<p>Given our results in <span class="math notranslate nohighlight">\(P(L \mid T)\)</span> and <span class="math notranslate nohighlight">\(P(L^c \mid T)\)</span>, we can produce the following table:</p>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Event</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L^c\)</span></p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Prior</p></td>
<td><p>0.58</p></td>
<td><p>0.42</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Posterior</p></td>
<td><p>0.33</p></td>
<td><p>0.67</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
<p>Let us answer our main statistical inquiry:</p>
<blockquote>
<div><p>As a single person in Vancouver, there is a 33% chance we will end up partnered <strong>IF</strong> we use Tinder. Moreover, there is a 67% chance we will <strong>NOT</strong> end up partnered <strong>IF</strong> we use Tinder.</p>
</div></blockquote>
<p>Now, our posterior knowledge of ending up partnered or not was updated with incorporating the information conditioned on the use of Tinder (the prior model <strong>DID NOT</strong> include that). Therefore, according to this posterior model, <strong>it is VERY DISHEARTENING to use Tinder to find a partner in Vancouver</strong>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>This is Bayesian inference!</strong></p>
<ol class="arabic simple">
<li><p>Hidden variables of interest are <strong>random</strong> (the <strong>prior distribution</strong>).</p></li>
<li><p>We use the <strong>conditional distribution</strong> of hidden variables given observations (the <strong>posterior distribution</strong>) to capture uncertainty.</p></li>
</ol>
</div>
<p><strong>Formal Definition of Bayes’ Rule</strong></p>
<p>Let <span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(T\)</span> be two events of interest, the posterior probability of <span class="math notranslate nohighlight">\(L\)</span> given <span class="math notranslate nohighlight">\(T\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[P(L \mid T) = \frac{P(L \cap T)}{P(T)} = \frac{P(L) P(T \mid L)}{P(T)},\]</div>
<p>where the total probability is</p>
<div class="math notranslate nohighlight">
\[\begin{align*}
P(T) &amp;= [P(T \mid L) \times P(L)] + [P(T \mid L^c) \times P(L^c)].
\end{align*}\]</div>
<p>In general, for any continous or discrete case, this is defined as:</p>
<div class="math notranslate nohighlight" id="equation-full-bayes-rule">
<span class="eqno">(4)<a class="headerlink" href="#equation-full-bayes-rule" title="Permalink to this equation">#</a></span>\[\text{posterior} = \frac{\text{prior} \times \text{likelihood}}{\text{normalizing constant}}\]</div>
<div class="math notranslate nohighlight">
\[\text{posterior} \propto \text{prior} \times \text{likelihood}\]</div>
<p><strong>We know the likelihood and prior, they are our generative model!</strong></p>
<p><strong>Once you have the posterior, you have everything!</strong></p>
<p>The posterior distribution is … well, a distribution! So we can use it for:</p>
<ul class="simple">
<li><p><strong>Point Estimates:</strong> mean, mode, median.</p></li>
<li><p><strong>Uncertainty:</strong> variance, credible intervals.</p></li>
<li><p><strong>Prediction:</strong> generate more data given previous data.</p></li>
</ul>
<p>Our description of uncertainty is intuitive; it is a probability distribution over the parameters we do not know!</p>
</section>
<section id="rule-of-independent-events">
<h3>2.7. Rule of Independent Events<a class="headerlink" href="#rule-of-independent-events" title="Permalink to this heading">#</a></h3>
<p>Once we have defined the Bayes’ rule, we can proceed with the rule of independent events. Aside from our previous example, assume that event <span class="math notranslate nohighlight">\(T\)</span> has <strong>no effect</strong> on event <span class="math notranslate nohighlight">\(L\)</span>. We can represent this matter as follows:</p>
<div class="math notranslate nohighlight">
\[P(L \mid T) = P(L).\]</div>
<p><strong>If the statement above holds</strong>, by the Bayes’ rule, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(T \mid L) &amp;= \frac{P(T \cap L)}{P(L)} = \frac{P(L \mid T) P(T)}{P(L)} \\
&amp;= \frac{P(L) P(T)}{P(L)} \qquad \qquad \text{since } P(L \mid T) = P(L)\\
&amp;= P(T).
\end{align*}\end{split}\]</div>
<p>Again, using the Bayes’ rule, how can we obtain <span class="math notranslate nohighlight">\(P(T \cap L)\)</span>? We have that:</p>
<div class="math notranslate nohighlight">
\[P(T \cap L) = P(T \mid L) P(L).\]</div>
<p>And we previously derived that:</p>
<div class="math notranslate nohighlight">
\[P(T \mid L) = P(T).\]</div>
<p>Therefore, two events (<span class="math notranslate nohighlight">\(L\)</span> and <span class="math notranslate nohighlight">\(T\)</span>) are statistically independent if</p>
<div class="math notranslate nohighlight">
\[P(T \cap L)  = P(T) \times P(L).\]</div>
</section>
</section>
<section id="maximum-a-posteriori-estimation-map-and-maximum-likelihood-estimation-mle">
<h2>3. Maximum a Posteriori Estimation (MAP) and Maximum Likelihood Estimation (MLE)<a class="headerlink" href="#maximum-a-posteriori-estimation-map-and-maximum-likelihood-estimation-mle" title="Permalink to this heading">#</a></h2>
<p><strong>Maximum a posteriori (MAP) estimation is the Bayesian version of maximum likelihood estimation (MLE).</strong> Let us compare both approaches:</p>
<ul class="simple">
<li><p>MLE aims to find the value that maximizes the sample’s likelihood function (i.e., we are just relying on our observed sampled data!).</p></li>
<li><p>MAP aims to obtain the value that maximizes our posterior distribution, which was obtained with our <strong>observed data</strong> (i.e., our evidence via the likelihood) weighted by our <strong>prior probability model</strong>. Recall the fundamental result for our posterior:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{posterior} \propto \text{prior} \times \text{likelihood}\]</div>
<p>Mathematically, we can define the MAP as:</p>
<div class="math notranslate nohighlight">
\[\theta^\star_{\text{MAP}} = \arg\!\max_\theta f_{\theta | \mathbf{x}}(\theta)\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{\theta | \mathbf{x}}(\theta)\)</span> is the posterior distribution of our parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span> <strong>given</strong> our observed data <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \dots, x_n)'\)</span> (the likelihood) weighted by our prior belief on <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>In practice, using the posterior probability distribution of our parameter of interest, we can obtain the MAP:</p>
<ul class="simple">
<li><p>It will be the value associated with the maximum posterior probability in the case of a continuous population parameter of interest.</p></li>
<li><p>It will be the category with the largest posterior probability in discrete variables of this class. For instance, the event <strong>not ending up partnered (<span class="math notranslate nohighlight">\(L^c\)</span>)</strong> in the Tinder example.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Event</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(L^c\)</span></p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Prior</p></td>
<td><p>0.58</p></td>
<td><p>0.42</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Posterior</p></td>
<td><p>0.33</p></td>
<td><p>0.67</p></td>
<td><p>1</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="wrapping-up">
<h2>4. Wrapping Up<a class="headerlink" href="#wrapping-up" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Conditional probability is necessary to get into the Bayes’ rule.</p></li>
<li><p>In Bayesian inference, we rely on two key components to obtain our posterior probability: the likelihood (evidence provided by our data) and the prior probability model for our parameter of interest (our prior belief).</p></li>
<li><p>The Bayes’ rule applies to both discrete and continuous random variables.</p></li>
<li><p>In Bayesian inference, MAP is the replacement of MLE.</p></li>
</ul>
</section>
<section id="probability-puzzle-if-time-permits">
<h2>5. Probability Puzzle (if time permits)<a class="headerlink" href="#probability-puzzle-if-time-permits" title="Permalink to this heading">#</a></h2>
<p>The following puzzle was taken from <a class="reference external" href="https://webcat.library.ubc.ca/vwebv/holdingsInfo?bibId=2519113">Casella and Berger (2002)</a>.</p>
<blockquote>
<div><p>When coded messages are sent, there are sometimes errors in transmission. In particular, Morse code uses “dots” and “dashes”, which are known to occur in the proportion of 3:4. This means that for any given symbol,</p>
<div class="math notranslate nohighlight">
\[P(\text{dot sent}) = \frac{3}{7} \quad \text{and} \quad P(\text{dash sent}) = \frac{4}{7}.\]</div>
<p>Suppose there is intereference on the transmission line, and with probability <span class="math notranslate nohighlight">\(\frac{1}{8}\)</span> a dot is mistakenly received as a dash, and vice versa. <strong>If we receive a dot, what is the probability that a dot was sent?</strong></p>
</div></blockquote>
<p><strong>Answer</strong></p>
<p>Using the so-called Bayes’ rule, we have the following:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(\text{dot sent} \mid \text{dot received}) &amp;= \frac{P(\text{dot sent} \cap \text{dot received})}{P(\text{dot received})} \\
&amp;= \frac{P(\text{dot received} \mid \text{dot sent}) P(\text{dot sent})}{P(\text{dot received})}.
\end{align*}\end{split}\]</div>
<p>We know the following:</p>
<div class="math notranslate nohighlight">
\[P(\text{dot sent}) = \frac{3}{7}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(\text{dot received} \mid \text{dot sent}) &amp;= 1 - P(\text{dash received} \mid \text{dot sent}) \\
&amp;= 1 - \frac{1}{8} = \frac{7}{8}.
\end{align*}\end{split}\]</div>
<p>We already have two probabilities. Now, how can we obtain <span class="math notranslate nohighlight">\(P(\text{dot received})\)</span>?</p>
<p>Let us apply the law of <strong>total probability</strong> we just saw in the Tinder example:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
P(\text{dot received}) &amp;= P(\text{dot received} \cap \text{dot sent}) + P(\text{dot received} \cap \text{dash sent})\\
&amp;= P(\text{dot received} \mid \text{dot sent}) P(\text{dot sent}) + (\text{dot received} \mid \text{dash sent}) P(\text{dash sent}) \\
&amp;= \frac{7}{8} \times \frac{3}{7} + \frac{1}{8} \times \frac{4}{7} = \frac{25}{56}.
\end{align*}\end{split}\]</div>
<p>Now, we have the three probabilities!</p>
<div class="math notranslate nohighlight">
\[P(\text{dot sent} \mid \text{dot received}) = \frac{P(\text{dot received} \mid \text{dot sent}) P(\text{dot sent})}{P(\text{dot received})} = \frac{(7/8) \times (3/7)}{25/56} = \frac{21}{25}.\]</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./notes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture1_intro_and_stan.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture 1 - Frequentist and Bayesian Overview, Probabilistic Generative Models, and <code class="docutils literal notranslate"><span class="pre">Stan</span></code></p>
      </div>
    </a>
    <a class="right-next"
       href="lecture3_beta_binomial_Bayesian_modelling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Lecture 3 - Bayesian Statistics in Action: The Beta-Binomial Model</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#today-s-learning-objectives">Today’s Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-next">0. What is next?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">1. Conditional Probability</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule-fundamentals">2. Bayes’ Rule Fundamentals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-statistical-inquiry">2.1. Main Statistical Inquiry</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-data">2.2. The Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-prior">2.3. The Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability-and-likelihood">2.4. Conditional Probability and Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalizing-constant">2.5. Normalizing Constant</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-rule-and-posterior-probability">2.6. Bayes’ Rule and Posterior Probability</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rule-of-independent-events">2.7. Rule of Independent Events</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-a-posteriori-estimation-map-and-maximum-likelihood-estimation-mle">3. Maximum a Posteriori Estimation (MAP) and Maximum Likelihood Estimation (MLE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#wrapping-up">4. Wrapping Up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-puzzle-if-time-permits">5. Probability Puzzle (if time permits)</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By G. Alexi Rodríguez-Arelis, Hedayat Zarkoob, Michael Gelbart, and Trevor Campbell
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>